#!/bin/bash

function run {
  echo -e "\033[90m$\033[0m $1"
  eval "$1"
  EVAL_EXIT_STATUS=$?

  if [[ $EVAL_EXIT_STATUS -ne 0 ]]; then
    exit $EVAL_EXIT_STATUS
  fi
}

echo "~~~ :mag: Testing connection to the login node \"$SUPERCLUSTER_LOGIN_HOST\""

# Do the first check with BatchMode=yes (which won't prompt for password access)
SUPERCLUSTER_NAME=$(ssh -oBatchMode=yes "$SUPERCLUSTER_LOGIN_HOST" "echo \"\$(whoami)@\$(hostname)\"")
SSH_TEST_EXIT_STATUS=$?

# If the SSH connection test failed, we should just bail now
if [[ $SSH_TEST_EXIT_STATUS -ne 0 ]]; then
  echo "SSH connection test exited with $SSH_TEST_EXIT_STATUS"
  echo $SUPERCLUSTER_HOSTNAME
  exit $SSH_TEST_EXIT_STATUS
fi

echo "âœ… Connection to $SUPERCLUSTER_LOGIN_HOST is all good!"

# Since we did that test, we can use the information we got back to construct a
# nice display name for prompts
SUPERCLUSTER_PROMPT="\033[90m${SUPERCLUSTER_NAME}:$\033[0m"

# Come up with the place that the repository will be checked out to
SANITIZED_AGENT_NAME=$(echo "$BUILDKITE_AGENT_NAME" | tr -d '"')
PIPELINE_FOLDER_NAME="$SANITIZED_AGENT_NAME/$BUILDKITE_ORGANIZATION_SLUG/$BUILDKITE_PIPELINE_SLUG"
export SUPERCLUSTER_CHECKOUT_FOLDER=".buildkite/$PIPELINE_FOLDER_NAME"

# The job name is suffixed with the current PID (so if a job is retried with te
# same $BUILDKITE_JOB_ID, when we check job statuses it won't return the
# previous jobs run status)
SUPERCLUSTER_JOB_NAME="buildkite-job-${BUILDKITE_JOB_ID}-$$"

SUPERCLUSTER_BOOTSTRAP_SCRIPT_NAME="buildkite-bootstrap-${BUILDKITE_JOB_ID}.sh"
SUPERCLUSTER_RUNNER_SCRIPT_NAME="buildkite-runner-${BUILDKITE_JOB_ID}.sh"

# The command script will run the desired command from Buildkite within the compute node
SUPERCLUSTER_COMMAND_SCRIPT_NAME="buildkite-exec-${BUILDKITE_JOB_ID}.sh"
SUPERCLUSTER_COMMAND_LOG_NAME="buildkite-output-${BUILDKITE_JOB_ID}.log"
SUPERCLUSTER_EXIT_STATUS_FILE="buildkite-exit-status-${BUILDKITE_JOB_ID}"

read -r -d '' SUPERCLUSTER_COMMAND_SCRIPT << EOM
#!/bin/bash
set -e
if [[ -f "${BUILDKITE_COMMAND}" ]]; then
  chmod +x "${BUILDKITE_COMMAND}"
  ./"${BUILDKITE_COMMAND}"
else
  ${BUILDKITE_COMMAND}
fi
EOM

# The runner script is what is submitted to the cluster. It will run the
# command script, stream the output, and store the exit status.
read -r -d '' SUPERCLUSTER_RUNNER_SCRIPT << EOM
#!/bin/bash
./${SUPERCLUSTER_COMMAND_SCRIPT_NAME} >"${SUPERCLUSTER_COMMAND_LOG_NAME}" 2>&1
echo \$? > "${SUPERCLUSTER_EXIT_STATUS_FILE}"
EOM

# The bootstrap script is what is run on the remote login node, that prepares
# it for running a buildkite job
cat > $SUPERCLUSTER_BOOTSTRAP_SCRIPT_NAME <<- EOM
echo 'Connection established to "${SUPERCLUSTER_LOGIN_HOST}"'

function run {
  echo -e "${SUPERCLUSTER_PROMPT} \$1"
  eval "\$1"
  EVAL_EXIT_STATUS=\$?

  if [[ \$EVAL_EXIT_STATUS -ne 0 ]]; then
    exit \$EVAL_EXIT_STATUS
  fi
}

echo '~~~ :package: Preparing job folder'

run "mkdir -p \"${SUPERCLUSTER_CHECKOUT_FOLDER}\""

export GIT_TERMINAL_PROMPT=0

run "cd \"${SUPERCLUSTER_CHECKOUT_FOLDER}\""

if [[ -d ".git" ]]; then
  run "git remote set-url origin \"${BUILDKITE_REPO}\""
else
  run "git clone -v -- \"${BUILDKITE_REPO}\" ."
fi

run "git clean -fdqx"
run "git submodule foreach --recursive git clean \"-fdqx\""

if [[ -n "${BUILDKITE_REFSPEC:-}" ]]; then
  run "git fetch -v origin \"${BUILDKITE_REFSPEC}\""
  run "git checkout -f \"${BUILDKITE_COMMIT}\""
elif [[ "${BUILDKITE_PULL_REQUEST}" != "false" ]] && [[ "${BUILDKITE_PROJECT_PROVIDER}" == *"github"* ]]; then
  run "git fetch -v origin \"refs/pull/${BUILDKITE_PULL_REQUEST}/head\""
  run "git checkout -f \"${BUILDKITE_COMMIT}\""
elif [[ "${BUILDKITE_COMMIT}" == "HEAD" ]]; then
  run "git fetch -v origin \"${BUILDKITE_BRANCH}\""
  run "git checkout -f FETCH_HEAD"
else
  run "git fetch -v origin \"${BUILDKITE_COMMIT}\" || git fetch -v origin \"\$(git config remote.origin.fetch)\" \"+refs/tags/*:refs/tags/*\""
  run "git checkout -f \"${BUILDKITE_COMMIT}\""
fi

run "git submodule update --init --recursive"
run "git submodule foreach --recursive git reset --hard"

cat > "$SUPERCLUSTER_RUNNER_SCRIPT_NAME" <<- BEOM
${SUPERCLUSTER_RUNNER_SCRIPT}
BEOM

chmod +x "$SUPERCLUSTER_RUNNER_SCRIPT_NAME"

cat > "$SUPERCLUSTER_COMMAND_SCRIPT_NAME" <<- BEOM
${SUPERCLUSTER_COMMAND_SCRIPT}
BEOM

chmod +x "$SUPERCLUSTER_COMMAND_SCRIPT_NAME"

echo '~~~ :floppy_disk: Submitting job to the cluster'
run "sbatch --workdir=\"\$(pwd)\" --job-name=\"${SUPERCLUSTER_JOB_NAME}\" \"${SUPERCLUSTER_RUNNER_SCRIPT_NAME}\""
EOM

echo '~~~ :wrench: Connecting to the login node'

ssh "$SUPERCLUSTER_LOGIN_HOST" "bash -s" < "$SUPERCLUSTER_BOOTSTRAP_SCRIPT_NAME"
SSH_BOOTSTRAP_EXIT_STATUS=$?

# If the bootstrapping process failed, bad things must have happened...
if [[ $SSH_BOOTSTRAP_EXIT_STATUS -ne 0 ]]; then
  exit $SSH_BOOTSTRAP_EXIT_STATUS
fi

echo '~~~ :hourglass: Waiting for super cluster job to start'

echo "This could take a while..."

# The job status check command will grab the state for the current job (and
# make sure it has no leading or trailing whitespace)
JOB_STATUS_CHECK_COMMAND="sacct --name=\"${SUPERCLUSTER_JOB_NAME}\" --noheader -o \"state\" -X | tr -d ' '"
JOB_STATUS=""
IS_RUNNING=false

# Run in a loop so if for some reason SSH disconnects, it'll retry the wait
# once it's done. The possible states for a slurm job are:
# RUNNING, RESIZING, SUSPENDED, COMPLETED, CANCELLED, FAILED, TIMEOUT, PREEMPTED, BOOT_FAIL or NODE_FAIL
while true; do
  # Grab the current job status
  NEW_JOB_STATUS=$(ssh "$SUPERCLUSTER_LOGIN_HOST" "$JOB_STATUS_CHECK_COMMAND")
  SSH_GET_STATUS_EXIT_STATUS=$?

  if [[ "$NEW_JOB_STATUS" != "" ]]; then
    echo "ðŸ• Status has changed to \"$NEW_JOB_STATUS\" ($(date))"
  fi

  # If the SSH call fails, wait a while and try the loop again
  if [[ $SSH_GET_STATUS_EXIT_STATUS -ne 0 ]]; then
    echo "SSH exited with status $SSH_GET_STATUS_EXIT_STATUS - retrying in 60 seconds..."
    sleep 60
    continue
  fi

  # Oh, it's running now!?
  if [[ "$NEW_JOB_STATUS" == *"RUNNING"* ]] && [[ "$IS_RUNNING" = false ]]; then
    echo "~~~ :runner: Running on super cluster"
    IS_RUNNING=true
  fi

  # Woo! It's done!
  if [[ "$NEW_JOB_STATUS" == *"COMPLETED"* ]] \
    || [[ "$NEW_JOB_STATUS" == *"CANCELLED"* ]] \
    || [[ "$NEW_JOB_STATUS" == *"FAILED"* ]] \
    || [[ "$NEW_JOB_STATUS" == *"TIMEOUT"* ]] \
    || [[ "$NEW_JOB_STATUS" == *"BOOT_FAIL"* ]] \
    || [[ "$NEW_JOB_STATUS" == *"NODE_FAIL"* ]]; then
    echo "~~~ :thumbsup: Finished on super cluster"
    break
  fi

  JOB_STATUS="$NEW_JOB_STATUS"

  # Monitor the status of the job until it changes
  ssh "$SUPERCLUSTER_LOGIN_HOST" "bash -s" << EOM
  while true; do
    THIS_JOB_STATUS=\$($JOB_STATUS_CHECK_COMMAND)
    if [[ "\$THIS_JOB_STATUS" != "" ]] && [[ "\$THIS_JOB_STATUS" != "${JOB_STATUS}" ]]; then
      exit 0
    fi
    sleep 1
  done
EOM
done

echo "Downloading logs and artifacts from the login node"

run "scp \"${SUPERCLUSTER_LOGIN_HOST}\":\"${SUPERCLUSTER_CHECKOUT_FOLDER}/${SUPERCLUSTER_COMMAND_LOG_NAME} ${SUPERCLUSTER_CHECKOUT_FOLDER}/${SUPERCLUSTER_EXIT_STATUS_FILE}\" ."

echo "+++ :notebook_with_decorative_cover: Results from \`$BUILDKITE_COMMAND\`"

cat "${SUPERCLUSTER_COMMAND_LOG_NAME}"

exit $(cat "${SUPERCLUSTER_EXIT_STATUS_FILE}")
